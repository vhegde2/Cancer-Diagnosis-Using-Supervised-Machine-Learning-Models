{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d028d3f1-8c42-4c21-9f9f-5284661778cc",
   "metadata": {},
   "source": [
    "# Cancer Diagnosis Using Supervised Machine Learning Models\n",
    "\n",
    "## Project 3\n",
    "\n",
    "### Vindhya Hegde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528e741-663e-4555-9988-797de2b43b95",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Model Metric and Splitting the data](#Model-Metric-and-Splitting-the-data)\n",
    "- [Models](#Models)\n",
    "- [Model Fitting and Model Testing](#Model-Fitting-and-Model-Testing)\n",
    "- [Summary of the five models](#Summary-of-the-five-models)\n",
    "- [Sources](#Sources)\n",
    "- [Author](#Author)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614237b-31d7-4d0d-948e-e5c1d6972b87",
   "metadata": {},
   "source": [
    "# Introduction: \n",
    "\n",
    "### What is Supervised learning and why it is used?\n",
    "\n",
    "**Supervised learning** is a type of machine learning in which an algorithm is trained on a set of input and output data in order to anticipate output labels. Using the patterns discovered from training data, supervised learning aims to generate precise predictions on new and unforeseen data.\n",
    "\n",
    "By improving the model’s parameters, supervised learning aims to reduce the discrepancy between the outputs of training data and those predicted by the model. The model is then tested on a different data in order to gauge its performance and effectiveness. The major benefit of supervised learning is that it gives precise classifications and predictions of the incoming data based on historical patterns in the training data.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "For this project, the dataset used is **Cancer Data** Dataset from Kaggle (https://www.kaggle.com/datasets/erdemtaha/cancer-data). The dataset includes data on 570 cancer cells and 30 features. These features describe characteristics of the cell samples, such as texture, radius, and perimeter, which can be used to distinguish between benign and malignant cells.\n",
    "\n",
    "### Objective of this project\n",
    "\n",
    "The objective of this project is to determine if a cancer is benign or malignant, and the dataset contains features calculated from scanned images of samples. The aim of this project is to develop machine learning models that can correctly predict if the cancer tumor is malignant or benign based on specific characteristics.\n",
    "By using supervised learning models, the goal is to develop a tool that can give rapid and accurate diagnosis of the cancer. For that we can develop predictive model that can correctly categorize new tumors as either benign or malignant by training it on the labeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6e196-032c-4858-8f43-6618e7f03bf7",
   "metadata": {},
   "source": [
    "# Model Metric and Splitting the data\n",
    "\n",
    "**Model Metrics**\n",
    "\n",
    "A machine learning model’s performance is assessed using model metrics. **Area Under the ROC Curve (AUC-ROC)** and **Accuracy** are the two model metrics used in this project to evaluate the models.\n",
    "\n",
    "AUC-ROC measures the model’s capacity to distinguish between positive and negative classes, with the performance of the model being indicated by the area under the curve. The benefit of that it is not affected by class imbalance and offers just one model performance indicator. However, if the costs of false positive and false negatives differ, it might not be the ideal metric to use.\n",
    "\n",
    "Another metric used in this project is Accuracy. Accuracy calculates the percentage of the model’s total predictions that were accurate. Accuracy has several benefits, including simplicity, but if there is a class imbalance in the dataset, it may be misleading. A model that consistently predicts the majority class will have a 95% accuracy, but it won’t be useful if the dataset contains 95% of one class and 5% of the other.\n",
    "\n",
    "**Splitting Data**\n",
    "\n",
    "Machine learning frequently involves dividing the data into a training set and a test set, and this is done to avoid overfitting and to estimate model's performance.\n",
    "\n",
    "In order to avoid overfitting, a machine learning model should not be trained on the complete dataset because it is likely to memorize the training data rather than discover the underlying patterns. As a result, the model overfits, which causes it to perform well on the training set but poorly on the test set. We can train the model on one set and assess its performance on the other set, preventing overfitting, by dividing the data into a training set and a test set. In general, it is essential to divide the data into a training set and a test set before creating a machine learning model that can generalize well to new and unseen data.\n",
    "\n",
    "The dataset is divided into training and testing set in order to assess how well the models perform. To evaluate the model’s performance on new and unseen data, the data is split into two parts, one for training the model and other for testing. **In this project, the data is divided into 80:20 ratio with 80% of the data used for training and 20% for testing. The data is randomly divided for this evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d597d-2bfe-428c-87e6-6b0bb83d3381",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "The five models used in this project are:\n",
    "\n",
    "+ Logistic Regression\n",
    "+ Decision Tree\n",
    "+ Random Forest\n",
    "+ Gradient Boosting\n",
    "+ Support Vector Machine\n",
    "\n",
    "Based on the unique features of the tumor, these models can be used to determine whether a breast tumor is benign or malignant. These models are taught to recognize the patterns that differentiate between benign and malignant tumors and can then be trained to make precise predictions on new and unlabeled data.\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "Logistic regression is a statistical method which is used to predict whether a result will fall into one of the two categories which is known as binary outcome (either 0 or 1). To do this, it estimates the probability that the outcome will be one of two options using information about input factors, the predictors. An S-shaped curve, a graph created by logistic regression, displays how the probability of a result changes as the predictor factor change. When attempting to predict whether something belongs to one class or another in machine learning, logistic regression is often used.\n",
    "\n",
    "\n",
    "**Decision Tree:**\n",
    "\n",
    "Decision Tree is a machine learning algorithm which is used to make predictions or choices. It divides the predictor space into parts and offers predictions for each region in order to predict. It separates it into more parts, represented by tree branches. After each branch, a forecast is made, leading to more decisions or branches.\n",
    "Decision trees are utilized in many industries since they are easy to understand. It can be applied for classification and regression problems and can be performed on both numerical and categorical data. They are vulnerable to overfitting, which occurs when the tree becomes extremely complex in the training set and performs badly on unanticipated new input.\n",
    "\n",
    "**Random Forest:**\n",
    "\n",
    "Random Forest is a machine learning technique that combines multiple decision trees to provide a prediction. The model constructs a forest of decision trees, each of which is trained using a randomly chosen subset of the data. The individual trees are trained to predict outcomes based on different subsets of the data, and the final prediction is produced by combining the predictions of all trees.\n",
    "Building multiple trees aims to increase the model’s accuracy and robustness while minimizing the risk of overfitting, which occurs when a model performs well on training data and poorly on unforeseen data. Both classification and regression problems can be solved by random forest.\n",
    "\n",
    "**Gradient Boosting:** \n",
    "\n",
    "Gradient Boosting is a machine learning algorithm that successively combines numerous weak models to create a robust predictive model. It steadily improves the accuracy of the combined model by fitting a new model to mistakes made by the previous model in each phase. Until the model is capable of making accurate predictions, the procedure is repeated. \n",
    "Due to its capacity to manage complex non-linear interactions between features and target variables and its resistance to overfitting, gradient boosting has gained popularity. In conclusion, gradient boosting is a strong and adaptable technique for creating precise prediction models.\n",
    "\n",
    "\n",
    "**Support Vector Machines:**\n",
    "\n",
    "Support Vector Machine (SVM) is one of the machine learning models used for classification and regression analysis. It works by locating a line or hyperplane that best splits the data points into various categories. The hyperplane is selected to maximize the margin, or the separation between, the nearest data points for each class. \n",
    "As they are expert at handling complex data, SVMs are frequently employed for text classification, image classification, and other complex classification issues. SVMs are well known for being reliable and skilled at handling noisy data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e33ff9-4e27-43c7-bbb6-6193127059d1",
   "metadata": {},
   "source": [
    "# Model Fitting and Model Testing\n",
    "\n",
    "In this section, The five different classes of models were fit to the training data using Spark MLlib. \n",
    "This was done by using pipelines and cross validation for each model type. The measuring metrics were then used to assess the best model among each model type. \n",
    "\n",
    "The pipeline was set up in pyspark for each of models and the transformations were made using the functions from MLlib to easily put them into the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a2201-af00-4cfa-9dd2-e2908507b2a4",
   "metadata": {},
   "source": [
    "In the next code, imported necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "096fba56-ae83-4b29-8f4c-5884c0e51d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import SQLTransformer, StringIndexer, Binarizer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import PolynomialExpansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde2907-67fa-4126-9dd5-8045f1b1c1f9",
   "metadata": {},
   "source": [
    "Created a spark session and loaded the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57793e98-92f0-41ee-9e13-36ff9cb9154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CancerData\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"Cancer_Data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9e0a8-f828-42dc-9689-973b5bdc4aa7",
   "metadata": {},
   "source": [
    "Loaded and split the data into training and test sets. The dataset was split into 80% train and 20% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17184bf8-4459-4b18-97a0-6a2ba04807e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the data into training and test sets\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b4629-fca4-4473-92b3-74cc00a70b2e",
   "metadata": {},
   "source": [
    "The sequence of the transform were as follows\n",
    "\n",
    "+ `StringIndexer()`\n",
    "+ `Binarizer()`\n",
    "+ `SQLTransformer()`\n",
    "+ `VectorAssembler()`\n",
    "+ `PolynomialExpansion()`\n",
    "+ `StandardScaler()`\n",
    "\n",
    "This was done by first fitting the transformer using `.fit()` and then using `.transform()` to transform it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02530f-b8d0-4498-a1a1-7b32c822eca9",
   "metadata": {},
   "source": [
    "The response variable 'diagnosis' which was a categorical variable was converted into a binary variable in the following steps.\n",
    "\n",
    "In this code, doing the transformation `StringIndexer()` with inputCols = \"diagnosis\" and outputCols = \"diagnosis_numeric\" then fit and transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ad2fc-cf94-4f16-99c6-1706652f2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "indexer = StringIndexer(inputCols = [\"diagnosis\"], outputCols = [\"diagnosis_numeric\"])\n",
    "indexerTrans = indexer.fit(df) \n",
    "indexerTrans.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb081a-3f6a-4e5e-b5fe-9cc345d6e74c",
   "metadata": {},
   "source": [
    "In this code, doing the transformation `Binarizer()` with inputCol = \"diagnosis_numeric\" and outputCol = \"diagnosis_indicator\" then transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6020c-7dff-4d5c-8e64-9522bc6a3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryTrans = Binarizer(threshold = 0.5, inputCol = \"diagnosis_numeric\", outputCol = \"diagnosis_indicator\")\n",
    "binaryTrans.transform(indexerTrans.transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943462bf-4d11-4417-b900-273af33deb95",
   "metadata": {},
   "source": [
    "In this code, doing the transformation `SQLTransformer()` and select the features from the dataset and get diagnosis_indicator as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7cba58-238a-4b55-9b3d-23008913be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT radius_mean,\n",
    "texture_mean,\n",
    "perimeter_mean,\n",
    "area_mean,\n",
    "smoothness_mean,\n",
    "compactness_mean,\n",
    "concavity_mean,\n",
    "symmetry_mean,\n",
    "fractal_dimension_mean,\n",
    "radius_se,\n",
    "texture_se,\n",
    "perimeter_se,\n",
    "area_se,\n",
    "smoothness_se,\n",
    "compactness_se,\n",
    "concavity_se,\n",
    "symmetry_se,\n",
    "fractal_dimension_se,\n",
    "radius_worst,\n",
    "texture_worst,\n",
    "perimeter_worst,\n",
    "area_worst,\n",
    "smoothness_worst,\n",
    "compactness_worst,\n",
    "concavity_worst,\n",
    "symmetry_worst,\n",
    "fractal_dimension_worst,\n",
    "diagnosis_indicator as label FROM __THIS__\n",
    "\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae391b-b3c5-46a2-9320-98613e16dbc7",
   "metadata": {},
   "source": [
    "In this code, `SQLTransformer()` is transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93029c9d-7509-4b3f-9e38-7af6e710ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans.transform(\n",
    "    binaryTrans.transform(\n",
    "        indexerTrans.transform(df)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921913f9-1fe0-4c25-b859-81df16d3e7f6",
   "metadata": {},
   "source": [
    "In this code, doing the transformation `VectorAssembler()` outputCol = \"features\" then transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400ceaf-e617-43d2-9e17-e3e2a848c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['radius_mean',\n",
    "'texture_mean',\n",
    "'perimeter_mean',\n",
    "'area_mean',\n",
    "'smoothness_mean',\n",
    "'compactness_mean',\n",
    "'concavity_mean',\n",
    "'symmetry_mean',\n",
    "'fractal_dimension_mean',\n",
    "'radius_se',\n",
    "'texture_se',\n",
    "'perimeter_se',\n",
    "'area_se',\n",
    "'smoothness_se',\n",
    "'compactness_se',\n",
    "'concavity_se',\n",
    "'symmetry_se',\n",
    "'fractal_dimension_se',\n",
    "'radius_worst',\n",
    "'texture_worst',\n",
    "'perimeter_worst',\n",
    "'area_worst',\n",
    "'smoothness_worst',\n",
    "'compactness_worst',\n",
    "'concavity_worst',\n",
    "'symmetry_worst',\n",
    "'fractal_dimension_worst'], outputCol = \"features\")\n",
    "assembler.transform(sqlTrans.transform(binaryTrans.transform(indexerTrans.transform(df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af3f5b-794e-4c12-9848-ca477bdee1c0",
   "metadata": {},
   "source": [
    "In this code, doing the transformation `PolynomialExpansion()` with inputCol = \"features\" and outputCol = \"polyFeatures\" then transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c182f-0cac-4244-bd4f-1b878e443ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "polyExpansion = PolynomialExpansion(degree=3, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    "polyExpansion.transform(assembler.transform(sqlTrans.transform(binaryTrans.transform(indexerTrans.transform(df))))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab9dca-bf53-4c6e-a89b-2af29df460bb",
   "metadata": {},
   "source": [
    "In this code, doing the transformation `StandardScaler()` with inputCol=\"polyFeatures\"and outputCol=\"scaled_features\" then fit and transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92e5c7-4636-44b5-81b7-20a04802760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define StandardScaler for scaling the feature columns\n",
    "scaler = StandardScaler(inputCol=\"polyFeatures\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(polyExpansion.transform(assembler.transform(sqlTrans.transform(binaryTrans.transform(indexerTrans.transform(df))))))\n",
    "scalerModel.transform(polyExpansion.transform(assembler.transform(sqlTrans.transform(binaryTrans.transform(indexerTrans.transform(df))))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2185c-e55a-4be0-9ccf-d4143027b552",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9bacfd5-7466-4ddc-9b89-d4d47d5161e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184548b-8646-421d-a9a9-71e83173090f",
   "metadata": {},
   "source": [
    "In this code, **Logistic Regression Model** was built.\n",
    "\n",
    "The steps include:\n",
    "\n",
    "+ Defined the Logistic Regression model by `LogisticRegression()`\n",
    "+ Defined the pipeline and stages included transformations\n",
    "+ Defined the parameter grid for crossvaliadtion by using `ParamGridBuilder()` and `.addGrid()` to specify the tuning parameter values `regParam` and `elasticNetParam`. Then used the `.build()` method to build the grid.\n",
    "+ Defined the evaluator with `BinaryClassificationEvaluator()` with AUC-ROC metric and `MulticlassClassificationEvaluator()` with Accuracy metric\n",
    "+ Defined the cross validator\n",
    "+ Trained the model using cross validation\n",
    "+ Fit training data for AUC-ROC and Accuracy using `.fit()` \n",
    "+ Done predictions on test data using `.transform()`\n",
    "+ Calculated AUC-ROC and Accuracy on test data using `.evaluate()`\n",
    "+ Printed the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b469820a-b87e-4e13-bc22-807dbad73171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC-ROC: 0.9850746268656716\n",
      "Logistic Regression Accuracy: 0.9719626168224299\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
    "\n",
    "# pipeline with transformations and model\n",
    "#more 3 transformation after assembler\n",
    "pipeline_lr = Pipeline(stages = [indexerTrans, binaryTrans, sqlTrans, assembler, polyExpansion, scalerModel, lr])\n",
    "\n",
    "# ParamGrid for cross-validation\n",
    "param_grid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "evaluatorA_lr = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluatorB_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "\n",
    "crossvalA_lr = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=param_grid_lr,\n",
    "                          evaluator= evaluatorA_lr ,\n",
    "                          numFolds=5)\n",
    "\n",
    "crossvalB_lr = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=param_grid_lr,\n",
    "                          evaluator= evaluatorB_lr,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Fit training data for ROC\n",
    "cv_modelA_lr = crossvalA_lr.fit(train)\n",
    "\n",
    "\n",
    "# predictions on test data\n",
    "predictionsA_lr = cv_modelA_lr.bestModel\n",
    "predictionsA_lr = predictionsA_lr.transform(test)\n",
    "\n",
    "# Fit training data for accuracy\n",
    "cv_modelB_lr = crossvalB_lr.fit(train)\n",
    "\n",
    "# predictions on test data\n",
    "predictionsB_lr = cv_modelB_lr.bestModel\n",
    "predictionsB_lr = predictionsB_lr.transform(test)\n",
    "\n",
    "# Calculate ROC on test data\n",
    "roc_auc_lr = evaluatorA_lr.evaluate(predictionsA_lr)\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "accuracy_lr = evaluatorB_lr.evaluate(predictionsB_lr)\n",
    "\n",
    "# Print the results\n",
    "print(\"Logistic Regression AUC-ROC: {}\".format(roc_auc_lr))\n",
    "print(\"Logistic Regression Accuracy: {}\".format(accuracy_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873029b-0387-451b-8071-0f13c3f73be8",
   "metadata": {},
   "source": [
    "Calculating best regularization parameter value through `avgMetrics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "63cee6f1-93b9-4e7f-ae40-26088f1adc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.9882376532021245), (0.01, 0.9910330386565382)]\n",
      "[(0.0, 0.9882376532021245), (0.5, 0.9910330386565382), (1.0, 0.9906028880264973)]\n",
      "[(0.1, 0.9728179708245955), (0.01, 0.9573764330198704)]\n",
      "[(0.0, 0.9728179708245955), (0.5, 0.9573764330198704), (1.0, 0.9308762920215731)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([0.1, 0.01], cv_modelA_lr.avgMetrics)))\n",
    "print(list(zip([0.0, 0.5, 1.0], cv_modelA_lr.avgMetrics)))\n",
    "print(list(zip([0.1, 0.01], cv_modelB_lr.avgMetrics)))\n",
    "print(list(zip([0.0, 0.5, 1.0], cv_modelB_lr.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e085f-3c53-44e1-ba25-1160b0f25c2c",
   "metadata": {},
   "source": [
    "#### Interpretation of the Logistic Regression\n",
    "\n",
    "The accuracy of the logistic regression model on the test data is 0.972, and its AUC-ROC score is 0.985. According to the AUC-ROC score, the model does a decent job of differentiating between the positive and negative classifications. The model accurately classified 97.2% of the observations in the test set, according to the accuracy score.\n",
    "\n",
    "Based on the given list of average metrics for different values of regularization parameter, the highest metric value (0.9882376532021245) is associated with the regularization parameter of 0.1 and 0.0 for CV Model A and the highest metric value (0.9728179708245955) is associated with the regularization parameter of 0.1 and 0.0 for CV Model B. Therefore, it can be concluded that the best regularization parameter value for the model's performance is 0.1 and 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56a819-b753-4851-8bc5-86d002b05b98",
   "metadata": {},
   "source": [
    "In this code, **Decision Tree Model** was built.\n",
    "\n",
    "The steps include:\n",
    "\n",
    "+ Defined the Decision Tree model by `DecisionTreeClassifier()`\n",
    "+ Defined the pipeline and stages included transformations\n",
    "+ Defined the parameter grid for crossvaliadtion by using `ParamGridBuilder()` and `.addGrid()` to specify the tuning parameter values `maxDepth` and `minInstancesPerNode` . Then used the `.build()` method to build the grid.\n",
    "+ Defined the evaluator with `BinaryClassificationEvaluator()` with AUC-ROC metric and `MulticlassClassificationEvaluator()` with Accuracy metric\n",
    "+ Defined the cross validator\n",
    "+ Trained the model using cross validation\n",
    "+ Fit training data for AUC-ROC and Accuracy using `.fit()` \n",
    "+ Done predictions on test data using `.transform()`\n",
    "+ Calculated AUC-ROC and Accuracy on test data using `.evaluate()`\n",
    "+ Printed the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74d1a059-7192-44ac-a603-5eca61b8afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree AUC-ROC: 0.9738805970149252\n",
      "Decision Tree Accuracy: 0.9626168224299065\n"
     ]
    }
   ],
   "source": [
    "#DT\n",
    "\n",
    "# pipeline for dt\n",
    "dt = DecisionTreeClassifier(featuresCol=\"scaled_features\", labelCol=\"label\", maxDepth=5)\n",
    "pipeline_dt = Pipeline(stages = [indexerTrans, binaryTrans, sqlTrans, assembler, polyExpansion, scalerModel, dt])\n",
    "\n",
    "\n",
    "# ParamGrid\n",
    "param_grid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 5, 10])\\\n",
    "    .build()\n",
    "\n",
    "# CrossValidator\n",
    "\n",
    "evaluatorA_dt = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluatorB_dt = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "\n",
    "crossvalA_dt = CrossValidator(estimator=pipeline_dt,\n",
    "                          estimatorParamMaps=param_grid_dt,\n",
    "                          evaluator= evaluatorA_dt,\n",
    "                          numFolds=5)\n",
    "\n",
    "crossvalB_dt = CrossValidator(estimator=pipeline_dt,\n",
    "                          estimatorParamMaps=param_grid_dt,\n",
    "                          evaluator= evaluatorB_dt,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Fit training data for ROC\n",
    "cv_modelA_dt = crossvalA_dt.fit(train)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictionsA_dt = cv_modelA_dt.transform(test)\n",
    "\n",
    "# Fit training data for accuracy\n",
    "cv_modelB_dt = crossvalB_dt.fit(train)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictionsB_dt = cv_modelB_dt.transform(test)\n",
    "\n",
    "# Calculate ROC on test data\n",
    "roc_auc_dt = evaluatorA_dt.evaluate(predictionsA_dt)\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "accuracy_dt = evaluatorB_dt.evaluate(predictionsB_dt)\n",
    "\n",
    "# Print the results\n",
    "print(\"Decision Tree AUC-ROC: {}\".format(roc_auc_dt))\n",
    "print(\"Decision Tree Accuracy: {}\".format(accuracy_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7713ce-29f9-4737-91e9-f3aa29f37544",
   "metadata": {},
   "source": [
    "Calculating best regularization parameter value through `avgMetrics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fd969b97-5242-4335-840a-29a2c4d360bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.9594897570942471), (5, 0.9538910744015713), (10, 0.9484870791629338)]\n",
      "[(2, 0.9594897570942471), (5, 0.9538910744015713), (10, 0.9484870791629338)]\n",
      "[(1, 0.9605921016818193), (5, 0.9635838865793488), (10, 0.9611147507768797)]\n",
      "[(2, 0.9605921016818193), (5, 0.9635838865793488), (10, 0.9611147507768797)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 5, 10], cv_modelA_dt.avgMetrics)))\n",
    "print(list(zip([2, 5, 10], cv_modelA_dt.avgMetrics)))\n",
    "print(list(zip([1, 5, 10], cv_modelB_dt.avgMetrics)))\n",
    "print(list(zip([2, 5, 10], cv_modelB_dt.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0925afa6-2aec-4db1-97cd-0a7f4dd25b86",
   "metadata": {},
   "source": [
    "#### Interpretation of the Decision Tree model\n",
    "\n",
    "The accuracy of the decision tree model on the test data is 0.9738, and its AUC-ROC score is 0.9626. According to the AUC-ROC score, the model does a decent job of differentiating between the positive and negative classifications. The model accurately classified 96.26% of the observations in the test set, according to the accuracy score.\n",
    "\n",
    "Based on the given list of average metrics for different values of regularization parameter, the highest metric value (0.9594897570942471) is associated with the regularization parameter of 1 and 2 for CV Model A and the highest metric value (0.9605921016818193) is associated with the regularization parameter of 1 and 2 for CV Model B. Therefore, it can be concluded that the best regularization parameter value for the model's performance is 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4c6bb-a87b-4fdd-b104-3ec32e100712",
   "metadata": {},
   "source": [
    "In this code, **Random Forest Model** was built.\n",
    "\n",
    "The steps include:\n",
    "\n",
    "+ Defined the Random Forest model by `RandomForestClassifier()`\n",
    "+ Defined the pipeline and stages included transformations\n",
    "+ Defined the parameter grid for crossvaliadtion by using `ParamGridBuilder()` and `.addGrid()` to specify the tuning parameter values `maxDepth` and `numTrees` . Then used the `.build()` method to build the grid.\n",
    "+ Defined the evaluator with `BinaryClassificationEvaluator()` with AUC-ROC metric and `MulticlassClassificationEvaluator()` with Accuracy metric\n",
    "+ Defined the cross validator\n",
    "+ Trained the model using cross validation\n",
    "+ Fit training data for AUC-ROC and Accuracy using `.fit()` \n",
    "+ Done predictions on test data using `.transform()`\n",
    "+ Calculated AUC-ROC and Accuracy on test data using `.evaluate()`\n",
    "+ Printed the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "712352cc-6c6f-4728-80f9-74cf592f15d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC-ROC: 0.9977611940298508\n",
      "Random Forest Accuracy: 0.9719626168224299\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "# pipeline for RF\n",
    "RF = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"label\", maxDepth=5, numTrees=200)\n",
    "pipeline_RF = Pipeline(stages = [indexerTrans, binaryTrans, sqlTrans, assembler, polyExpansion, scalerModel, RF])\n",
    "\n",
    "# ParamGrid \n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(RF.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(RF.numTrees, [100, 150, 200]) \\\n",
    "    .build()\n",
    "\n",
    "# CrossValidator\n",
    "\n",
    "evaluatorA_rf = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluatorB_rf = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "\n",
    "crossvalA_rf = CrossValidator(estimator=pipeline_RF,\n",
    "                          estimatorParamMaps=param_grid_rf,\n",
    "                          evaluator= evaluatorA_rf ,\n",
    "                          numFolds=5)\n",
    "\n",
    "crossvalB_rf = CrossValidator(estimator=pipeline_RF,\n",
    "                          estimatorParamMaps=param_grid_rf,\n",
    "                          evaluator= evaluatorB_rf,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Fit training data for ROC\n",
    "cv_modelA_rf = crossvalA_rf.fit(train)\n",
    "\n",
    "# predictions on test data\n",
    "predictionsA_rf = cv_modelA_rf.transform(test)\n",
    "\n",
    "# Fit training data for accuracy\n",
    "cv_modelB_rf = crossvalB_rf.fit(train)\n",
    "\n",
    "#predictions on test data\n",
    "predictionsB_rf = cv_modelB_rf.transform(test)\n",
    "\n",
    "# Calculate ROC on test data\n",
    "roc_auc_rf = evaluatorA_rf.evaluate(predictionsA_rf)\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "accuracy_rf = evaluatorB_rf.evaluate(predictionsB_rf)\n",
    "\n",
    "# Print the results\n",
    "print(\"Random Forest AUC-ROC: {}\".format(roc_auc_rf))\n",
    "print(\"Random Forest Accuracy: {}\".format(accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd87680-6c35-48ee-8b74-6c63724a93a4",
   "metadata": {},
   "source": [
    "Calculating best regularization parameter value through `avgMetrics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "471942d9-374d-449e-9920-3f631ab7678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.9893281116166825), (5, 0.990327909272487), (10, 0.9897167607873119)]\n",
      "[(100, 0.9893281116166825), (150, 0.990327909272487), (200, 0.9897167607873119)]\n",
      "[(2, 0.9689596242816891), (5, 0.9708287831601938), (10, 0.9670904654031844)]\n",
      "[(100, 0.9689596242816891), (150, 0.9708287831601938), (200, 0.9670904654031844)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([2,5,10], cv_modelA_rf.avgMetrics)))\n",
    "print(list(zip([100, 150, 200], cv_modelA_rf.avgMetrics)))\n",
    "print(list(zip([2,5,10], cv_modelB_rf.avgMetrics)))\n",
    "print(list(zip([100, 150, 200], cv_modelB_rf.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894947f1-009f-475e-a71b-8d559544ec84",
   "metadata": {},
   "source": [
    "#### Interpretation of the Random Forest model\n",
    "\n",
    "The accuracy of the Random Forest model on the test data is 0.9976, and its AUC-ROC score is 0.9719. According to the AUC-ROC score, the model does a decent job of differentiating between the positive and negative classifications. The model accurately classified 97.19% of the observations in the test set, according to the accuracy score.\n",
    "\n",
    "Based on the given list of average metrics for different values of regularization parameter, the highest metric value (0.990327909272487) is associated with the regularization parameter of 5 and 150 for CV Model A and the highest metric value (0.9708287831601938) is associated with the regularization parameter of 5 and 150 for CV Model B. Therefore, it can be concluded that the best regularization parameter value for the model's performance is 5 and 150."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932043c1-2783-4793-a3a5-f35d67f8ac95",
   "metadata": {},
   "source": [
    "In this code, **Gradient Boosting** Model was built.\n",
    "\n",
    "The steps include:\n",
    "\n",
    "+ Defined the Gradient Boosting model by `GBTClassifier()`\n",
    "+ Defined the pipeline and stages included transformations\n",
    "+ Defined the parameter grid for crossvaliadtion by using `ParamGridBuilder()` and `.addGrid()` to specify the tuning parameter values `maxDepth` and `maxIter` . Then used the `.build()` method to build the grid.\n",
    "+ Defined the evaluator with `BinaryClassificationEvaluator()` with AUC-ROC metric and `MulticlassClassificationEvaluator()` with Accuracy metric\n",
    "+ Defined the cross validator\n",
    "+ Trained the model using cross validation\n",
    "+ Fit training data for AUC-ROC and Accuracy using `.fit()` \n",
    "+ Done predictions on test data using `.transform()`\n",
    "+ Calculated AUC-ROC and Accuracy on test data using `.evaluate()`\n",
    "+ Printed the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ece5a4c9-4d49-44b8-85cb-cc5386b39bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting AUC-ROC: 0.9977611940298508\n",
      "Gradient Boosting Accuracy: 0.9813084112149533\n"
     ]
    }
   ],
   "source": [
    "#GBT\n",
    "\n",
    "# pipeline for GBT\n",
    "gbt = GBTClassifier(featuresCol=\"scaled_features\", labelCol=\"label\", maxDepth=5, maxIter=20)\n",
    "pipeline_gbt = Pipeline(stages = [indexerTrans, binaryTrans, sqlTrans, assembler, polyExpansion, scalerModel, gbt])\n",
    "\n",
    "# ParamGrid \n",
    "param_grid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(gbt.maxIter, [10, 20, 30]) \\\n",
    "    .build()\n",
    "\n",
    "# CrossValidator\n",
    "\n",
    "evaluatorA_gbt = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "evaluatorB_gbt = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "\n",
    "crossvalA_gbt = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=param_grid_gbt,\n",
    "                          evaluator= evaluatorA_gbt ,\n",
    "                          numFolds=5)\n",
    "\n",
    "crossvalB_gbt = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=param_grid_gbt,\n",
    "                          evaluator= evaluatorB_gbt,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Fit training data for ROC\n",
    "cv_modelA_gbt = crossvalA_gbt.fit(train)\n",
    "\n",
    "# predictions on test data\n",
    "predictionsA_gbt = cv_modelA_gbt.transform(test)\n",
    "\n",
    "# Fit training data for accuracy\n",
    "cv_modelB_gbt = crossvalB_gbt.fit(train)\n",
    "\n",
    "# predictions on test data\n",
    "predictionsB_gbt = cv_modelB_gbt.transform(test)\n",
    "\n",
    "# Calculate ROC on test data\n",
    "roc_auc_gbt = evaluatorA_gbt.evaluate(predictionsA_gbt)\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "accuracy_gbt = evaluatorB_gbt.evaluate(predictionsB_gbt)\n",
    "\n",
    "# Print the results\n",
    "print(\"Gradient Boosting AUC-ROC: {}\".format(roc_auc_gbt))\n",
    "print(\"Gradient Boosting Accuracy: {}\".format(accuracy_gbt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc9f9d-4c90-429b-83d1-0881a7bcb6ab",
   "metadata": {},
   "source": [
    "Calculating best regularization parameter value through `avgMetrics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0cc1a461-e1de-4ff8-be70-439fc645f050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.9724045927578461), (5, 0.9705717603182034), (10, 0.973835901838824)]\n",
      "[(10, 0.9724045927578461), (20, 0.9705717603182034), (30, 0.973835901838824)]\n",
      "[(2, 0.9583449106705835), (5, 0.9605921016818193), (10, 0.9624612605603241)]\n",
      "[(10, 0.9583449106705835), (20, 0.9605921016818193), (30, 0.9624612605603241)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([2,5,10], cv_modelA_gbt.avgMetrics)))\n",
    "print(list(zip([10, 20, 30], cv_modelA_gbt.avgMetrics)))\n",
    "print(list(zip([2,5,10], cv_modelB_gbt.avgMetrics)))\n",
    "print(list(zip([10, 20, 30], cv_modelB_gbt.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b40ca-e5f7-4a3f-9276-1d15fd648f94",
   "metadata": {},
   "source": [
    "#### Interpretation of the Gradient Boosting model\n",
    "\n",
    "The accuracy of the Gradient Boosting model on the test data is 0.9977, and its AUC-ROC score is 0.9813. According to the AUC-ROC score, the model does a decent job of differentiating between the positive and negative classifications. The model accurately classified 98.13% of the observations in the test set, according to the accuracy score.\n",
    "\n",
    "Based on the given list of average metrics for different values of regularization parameter, the highest metric value (0.973835901838824) is associated with the regularization parameter of 10 and 30 for CV Model A and the highest metric value (0.9624612605603241) is associated with the regularization parameter of 10 and 30 for CV Model B. Therefore, it can be concluded that the best regularization parameter value for the model's performance is 10 and 30."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed1a3e-1337-4d61-91bb-02dcae4c4be9",
   "metadata": {},
   "source": [
    "In this code, **Support Vector Machine Model** was built.\n",
    "\n",
    "The steps include:\n",
    "\n",
    "+ Defined the Support Vector Machine model by `LinearSVC()`\n",
    "+ Defined the pipeline and stages included transformations\n",
    "+ Defined the parameter grid for crossvaliadtion by using `ParamGridBuilder()` and `.addGrid()` to specify the tuning parameter values `regParam` and `maxIter` . Then used the `.build()` method to build the grid.\n",
    "+ Defined the evaluator with `BinaryClassificationEvaluator()` with AUC-ROC metric and `MulticlassClassificationEvaluator()` with Accuracy metric\n",
    "+ Defined the cross validator\n",
    "+ Trained the model using cross validation\n",
    "+ Fit training data for AUC-ROC and Accuracy using `.fit()` \n",
    "+ Done predictions on test data using `.transform()`\n",
    "+ Calculated AUC-ROC and Accuracy on test data using `.evaluate()`\n",
    "+ Printed the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db37c9ab-9c86-4b5d-8dbd-1eb652984665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM AUC-ROC: 0.998134328358209\n",
      "SVM Accuracy: 0.9906542056074766\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "svm = LinearSVC(featuresCol=\"scaled_features\", maxIter=10, regParam=0.1)\n",
    "pipeline_svm = Pipeline(stages = [indexerTrans, binaryTrans, sqlTrans, assembler, polyExpansion, scalerModel, svm])\n",
    "\n",
    "# Create a ParamGridBuilder\n",
    "param_grid_svm = ParamGridBuilder() \\\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 1]) \\\n",
    "    .addGrid(svm.maxIter, [10, 100, 1000]) \\\n",
    "    .build()\n",
    "\n",
    "evaluatorA_svm = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluatorB_svm = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "\n",
    "crossvalA_svm = CrossValidator(estimator=pipeline_svm,\n",
    "                          estimatorParamMaps=param_grid_svm,\n",
    "                          evaluator= evaluatorA_svm,\n",
    "                          numFolds=5)\n",
    "\n",
    "crossvalB_svm = CrossValidator(estimator=pipeline_svm,\n",
    "                          estimatorParamMaps=param_grid_svm,\n",
    "                          evaluator= evaluatorB_svm,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Fit training data for ROC\n",
    "cv_modelA_svm = crossvalA_svm.fit(train)\n",
    "\n",
    "#predictions on test data\n",
    "predictionsA_svm = cv_modelA_svm.transform(test)\n",
    "\n",
    "# Fit training data for accuracy\n",
    "cv_modelB_svm = crossvalB_svm.fit(train)\n",
    "\n",
    "# predictions on test data\n",
    "predictionsB_svm = cv_modelB_svm.transform(test)\n",
    "\n",
    "# Calculate ROC on test data\n",
    "roc_auc_svm = evaluatorA_svm.evaluate(predictionsA_svm)\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "accuracy_svm = evaluatorB_svm.evaluate(predictionsB_svm)\n",
    "\n",
    "# Print the results\n",
    "print(\"SVM AUC-ROC: {}\".format(roc_auc_svm))\n",
    "print(\"SVM Accuracy: {}\".format(accuracy_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4063d7-5588-48e7-acf9-2a38af6651fe",
   "metadata": {},
   "source": [
    "Calculating best regularization parameter value through `avgMetrics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c2d5bfb-02ed-4fec-9818-be6bc3dd09cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.01, 0.9871438794315581), (0.1, 0.9830810106325607), (1, 0.9839606840433216)]\n",
      "[(10, 0.9871438794315581), (100, 0.9830810106325607), (1000, 0.9839606840433216)]\n",
      "[(0.01, 0.9708521585692635), (0.1, 0.9664544299236189), (1, 0.9668324620563503)]\n",
      "[(10, 0.9708521585692635), (100, 0.9664544299236189), (1000, 0.9668324620563503)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([0.01, 0.1, 1], cv_modelA_svm.avgMetrics)))\n",
    "print(list(zip([10, 100, 1000], cv_modelA_svm.avgMetrics)))\n",
    "print(list(zip([0.01, 0.1, 1], cv_modelB_svm.avgMetrics)))\n",
    "print(list(zip([10, 100, 1000], cv_modelB_svm.avgMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5b2d8-1920-4113-ad82-93ea7f4cd385",
   "metadata": {},
   "source": [
    "#### Interpretation of the Support Vector Machine model\n",
    "\n",
    "The accuracy of the SVM model on the test data is 0.9981, and its AUC-ROC score is 0.9906. According to the AUC-ROC score, the model does a decent job of differentiating between the positive and negative classifications. The model accurately classified 99.06% of the observations in the test set, according to the accuracy score.\n",
    "\n",
    "Based on the given list of average metrics for different values of regularization parameter, the highest metric value (0.9871438794315581) is associated with the regularization parameter of 0.01 and 10 for CV Model A and the highest metric value (0.9708521585692635) is associated with the regularization parameter of 0.01 and 10 for CV Model B. Therefore, it can be concluded that the best regularization parameter value for the model's performance is 0.01 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fb39c-4e59-4881-a0c5-0adbbbdd327b",
   "metadata": {},
   "source": [
    "# Summary of the five models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df15afba-83c4-462d-814e-e1f1edf2b791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression- AUC-ROC: 0.9850746268656716 Accuracy: 0.9719626168224299\n",
      "Decision Tree- AUC-ROC: 0.9738805970149252 Accuracy: 0.9626168224299065\n",
      "Random Forest- AUC-ROC: 0.9977611940298508 Accuracy: 0.9719626168224299\n",
      "Gradient Boosting- AUC-ROC: 0.9977611940298508 Accuracy: 0.9813084112149533\n",
      "Support Vector Machine- AUC-ROC: 0.998134328358209 Accuracy: 0.9906542056074766\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression- AUC-ROC:\", (roc_auc_lr), \"Accuracy:\", (accuracy_lr))\n",
    "print(\"Decision Tree- AUC-ROC:\", (roc_auc_dt), \"Accuracy:\", (accuracy_dt))\n",
    "print(\"Random Forest- AUC-ROC:\", (roc_auc_rf), \"Accuracy:\", (accuracy_rf))\n",
    "print(\"Gradient Boosting- AUC-ROC:\", (roc_auc_gbt), \"Accuracy:\", (accuracy_gbt))\n",
    "print(\"Support Vector Machine- AUC-ROC:\", (roc_auc_svm), \"Accuracy:\", (accuracy_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee71484-aafb-4ad6-93e8-d78e93d338f8",
   "metadata": {},
   "source": [
    "### Final Interpretation of Best Model\n",
    "\n",
    "The accuracy and AUC-ROC scores of all five models show that they are effective in determining whether a tumor is benign or malignant. \n",
    "\n",
    "With AUC-ROC values of **0.9977611940298508** and **0.998134328358209**, respectively, Gradient Boosting and Support Vector Machine are better able to differentiate between the two classes. \n",
    "\n",
    "The accuracy rating for the **Support Vector Machine** is **99.06%**. \n",
    "\n",
    "Logistic Regression, Decision Tree, and Random Forest, although they have lower AUC-ROC scores than the other two models 0.9850746268656716, 0.9738805970149252, and 0.9977611940298508, respectively, still perform well. \n",
    "\n",
    "Overall, each of the five models is a good choice for diagnosing cancer, but **Support Vector Machine** stands out as the best option as it has both **AUC-ROC** and **Accuracy** score higher than the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206b89f-7891-46c5-9eaf-ba0e5912b38c",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "https://www.kaggle.com/datasets/erdemtaha/cancer-data\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#regression\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aada50-a388-4595-80f1-8826da1dd824",
   "metadata": {},
   "source": [
    "# Author\n",
    "\n",
    "Vindhya Hegde for ST 590"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
